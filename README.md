
# ğŸš€ Real-Time Data Engineering Pipeline
### RandomUser API â†’ Kafka â†’ Spark Structured Streaming â†’ PostgreSQL

> **Production-style, end-to-end streaming pipeline** that ingests live data, processes it in real time, and persists clean, validated records for analytics.

---

## ğŸŒŸ Overview

This repository contains a **complete real-time data pipeline** designed to simulate
a production-grade Data Engineering system. The pipeline ingests live user data from
a public API, streams it through **Apache Kafka**, processes it using **Apache Spark
Structured Streaming**, and stores the results in **PostgreSQL**.

The project emphasizes **reliability, scalability, and fault tolerance**, covering
real-world concerns such as malformed data, schema validation, checkpointing, and
JDBC configuration.

---

## ğŸ§  Pipeline Architecture

### High-Level Flow

RandomUser API  
â†’ Python Producer  
â†’ Apache Kafka  
â†’ Spark Structured Streaming  
â†’ PostgreSQL

### Architecture Diagram

ğŸ“Œ **Add your pipeline drawing here**

![Pipeline Architecture](screenshots/pipeline-architecture.png)

> This diagram illustrates the full data flow from ingestion to persistence.

---

## ğŸ› ï¸ Tech Stack

| Layer | Tool |
|------|------|
| Data Source | RandomUser Public API |
| Ingestion | Python Kafka Producer |
| Messaging | Apache Kafka |
| Processing | Apache Spark Structured Streaming |
| Storage | PostgreSQL |
| Connectivity | JDBC |
| Environment | Linux Virtual Machine |

---

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ randomuser_producer.py
â”œâ”€â”€ spark_randomuser_to_postgres.py
â”œâ”€â”€ README.md
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ pipeline-architecture.png
â”‚   â”œâ”€â”€ kafka.png
â”‚   â”œâ”€â”€ producer-running.png
â”‚   â”œâ”€â”€ spark-streaming.png
â”‚   â””â”€â”€ postgres-table.png
â””â”€â”€ .gitignore
```

## How to Run

### 1ï¸âƒ£ Start Zookeeper

```javascript
zkServer.sh start

```
### 2ï¸âƒ£ Start Kafka Broker

```javascript
kafka-server-start.sh config/server.properties

```
### 3ï¸âƒ£ Create Kafka Topic

```javascript
kafka-topics.sh --create \
--topic randomuser-topic \
--bootstrap-server localhost:9092 \
--partitions 1 \
--replication-factor 1
```
### 4ï¸âƒ£ Run the Producer (API â†’ Kafka)

```javascript
python3 randomuser_producer.py

```
### 5ï¸âƒ£ Run Spark Structured Streaming

```javascript
spark-submit \
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 \
--jars postgresql-42.2.18.jar \
spark_randomuser_to_postgres.py


```
### 6ï¸âƒ£ Verify Data in PostgreSQL

```javascript
SELECT * FROM users LIMIT 10;
```


## ğŸ›¡ï¸ Data Quality & Reliability

The pipeline is designed with production-level reliability in mind: 

â†’ Filters malformed or invalid Kafka messages. 

â†’ Excludes records with missing critical fields.

â†’ Uses Spark checkpointing for fault tolerance and recovery .

â†’ Explicit JDBC driver configuration.

â†’ Proper database authentication and connection handling.

## âœ… Output

âœ” Continuous real-time ingestion.

âœ” Structured stream processing using Spark.

âœ” Clean and validated data stored in PostgreSQL.

âœ” Production-like streaming behavior.

## ğŸ¯ Key Learnings

Designing real-time streaming pipelines

Kafka producer and topic management

Spark Structured Streaming internals

Micro-batch processing with foreachBatch

JDBC integration with streaming jobs

Debugging and stabilizing streaming systems










## Author

- [@Rabha Gharib](https://github.com/RabhaGharib972)

